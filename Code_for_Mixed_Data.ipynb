{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a5fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import numpy as np\n",
    "import statistics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed52a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: takes single instance of multiple_nan in which distance is greater than thresholds, \n",
    "# Output: give the imputed instances\n",
    "\n",
    "def process(inst,mean_std,class_center):\n",
    "    \n",
    "    # we count number of nan in instance\n",
    "    dummy_df = inst.copy()\n",
    "    nan_count = dummy_df.isna().sum(axis =1)\n",
    "    \n",
    "    # we create number of instances as per the nan_count\n",
    "    dummy_df = pd.concat([dummy_df]*nan_count.values[0])\n",
    "    \n",
    "    # we get the columns in which nan is present \n",
    "    cols = dummy_df.columns[dummy_df.isna().any()]\n",
    "    \n",
    "    # we replace nan values with the mean_std values to a single cell in an instance\n",
    "    for i in range(len(cols)):\n",
    "        dummy_df.iloc[i,cols[i]] = mean_std.iloc[0,cols[i]]\n",
    "    \n",
    "    # we impute mean values to rest nan values\n",
    "    dummy_df.fillna(class_center,inplace = True)\n",
    "    \n",
    "    # we find the single instance with shortest distance from the class_center\n",
    "    dummy_distances = []\n",
    "    for i in range(len(cols)):\n",
    "        dummy_distances.append(math.dist(class_center, dummy_df.iloc[i]))\n",
    "    \n",
    "    min_index = dummy_distances.index(min(dummy_distances))\n",
    "    output = dummy_df.iloc[min_index]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "989a1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: data and class name\n",
    "# Output: complete and incomplete dataset of that particular class\n",
    "\n",
    "def data_split(data,c):\n",
    "    c_data = data[data[data.shape[1]-1]==c].dropna()\n",
    "    id = data[data[data.shape[1]-1]==c].isnull().any(axis =1)\n",
    "    ic_data = data[data[data.shape[1]-1] == c][id]\n",
    "    \n",
    "    return c_data,ic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fff3f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_original_data = pd.read_excel(r'give here original data location',header = None)\n",
    "fetched_data = pd.read_excel('give here location of data that has missing values',header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec7268ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating categorical values\n",
    "\n",
    "# getting categorical dataframe from the fetched_data\n",
    "cdata_1 = fetched_data.iloc[:,0]\n",
    "last_column = fetched_data.iloc[:,-1]\n",
    "fetched_categorical_data = pd.concat([cdata_1, last_column],axis=1)\n",
    "fetched_categorical_data = fetched_categorical_data.T.reset_index(drop=True).T\n",
    "\n",
    "# getting categorical dataframe from the fetched_original_data\n",
    "c_o_data_1 = fetched_original_data.iloc[:,0]\n",
    "fetched_original_categorical_data = pd.concat([c_o_data_1, last_column],axis=1)\n",
    "fetched_original_categorical_data = fetched_original_categorical_data.T.reset_index(drop=True).T\n",
    "\n",
    "# For categorical dataset run this cell\n",
    "\n",
    "enc = ce.ordinal.OrdinalEncoder(handle_missing = 'return_nan')\n",
    "\n",
    "original_data = enc.fit_transform(fetched_original_categorical_data)\n",
    "data = enc.transform(fetched_categorical_data)\n",
    "AE_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5a1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating Numerical values\n",
    "\n",
    "# getting numerical dataframe from the fetched_data\n",
    "ndata_1 = fetched_data.iloc[:,1:]\n",
    "fetched_numerical_data = pd.concat([ndata_1],axis=1)\n",
    "fetched_numerical_data = fetched_numerical_data.T.reset_index(drop=True).T\n",
    "\n",
    "# getting numerical dataframe from the fetched_origianl_data\n",
    "n_o_data_1 = fetched_original_data.iloc[:,1:]\n",
    "fetched_original_numerical_data = pd.concat([n_o_data_1],axis=1)\n",
    "fetched_original_numerical_data = fetched_original_numerical_data.T.reset_index(drop=True).T\n",
    "\n",
    "# For numerical dataset run this cell\n",
    "\n",
    "original_data = fetched_original_numerical_data.copy()\n",
    "data = fetched_numerical_data.copy()\n",
    "nrms_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aee1d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = np.unique(data[data.shape[1]-1])\n",
    "final_imputed_data = data.copy()\n",
    "final_imputed_data = final_imputed_data.iloc[0:0]\n",
    "\n",
    "for i in class_list:\n",
    "    \n",
    "    #-----------------------------first algoritms---------------------------------------------------------------------#\n",
    "    \n",
    "    # splitting the data into complete and incomplete data based on class\n",
    "    globals()[\"complete_data_\"+str(i)],globals()[\"incomplete_data_\"+str(i)] = data_split(data,i)\n",
    "    \n",
    "    #----------------------------sending the complete data to the final imputed data----------------------------------#\n",
    "    final_imputed_data = final_imputed_data.append(globals()[\"complete_data_\"+str(i)])\n",
    "    \n",
    "    # class center and sandard deviation\n",
    "    class_center = globals()[\"complete_data_\"+str(i)].mean()\n",
    "    std_deviation = globals()[\"complete_data_\"+str(i)].std()\n",
    "    \n",
    "    # threshold calculation\n",
    "    distances = []\n",
    "    for j in  globals()[\"complete_data_\"+str(i)].index:\n",
    "        distances.append(math.dist(class_center,  globals()[\"complete_data_\"+str(i)].loc[j])) \n",
    "        \n",
    "    threshold = statistics.median(distances)\n",
    "    \n",
    "    #-----------------------------second algoritms-------------------------------------------------------------------#\n",
    "    \n",
    "    # addition of std and mean for the step2 of second algorithm\n",
    "    mean_list=globals()[\"complete_data_\"+str(i)].mean().tolist()\n",
    "    std_list=globals()[\"complete_data_\"+str(i)].std().tolist()\n",
    "    final_list=[mean_list, std_list]\n",
    "    mean_std=[sum(x) for x in zip(*final_list)]\n",
    "    mean_std = pd.DataFrame(np.array(mean_std).reshape(-1,len(mean_std)),columns = globals()[\"complete_data_\"+str(i)].columns)\n",
    "\n",
    "    # splitting the incomplete data into single_nan and multiple_nan\n",
    "    globals()['single_nan_'+str(i)] = globals()[\"incomplete_data_\"+str(i)][globals()[\"incomplete_data_\"+str(i)].isna().sum(axis =1) ==1]\n",
    "    globals()['multiple_nan_'+str(i)] = globals()[\"incomplete_data_\"+str(i)][globals()[\"incomplete_data_\"+str(i)].isna().sum(axis =1) >1]\n",
    "    \n",
    "    # imputing the class center values to the single_nan and multiple_nan\n",
    "    globals()['single_nan_imp_'+str(i)]=globals()['single_nan_'+str(i)].fillna(class_center)\n",
    "    globals()['multiple_nan_imp_'+str(i)]=globals()['multiple_nan_'+str(i)].fillna(class_center)\n",
    "    \n",
    "    # comparing the distance with the threshold, if distance greater we use the above mentioned function for the imputation\n",
    "    # Also a new variable multiple_nan_imp_ created for the new instances\n",
    "    for j in globals()['multiple_nan_imp_'+str(i)].index:\n",
    "        new_distance = math.dist(class_center, globals()['multiple_nan_imp_'+str(i)].loc[j])\n",
    "        if new_distance > threshold:\n",
    "            globals()['multiple_nan_imp_'+str(i)].loc[j] = process(globals()['multiple_nan_'+str(i)].loc[[j]],mean_std,class_center)\n",
    "\n",
    "    # comparing the distance with the threshold, if distance greater we use the above mentioned function for the imputation\n",
    "    # Also a new variable single_nan_imp_ created for the new instances\n",
    "    for j in globals()['single_nan_imp_'+str(i)].index:\n",
    "        new_distance = math.dist(class_center, globals()['single_nan_imp_'+str(i)].loc[j])\n",
    "        if new_distance > threshold:\n",
    "            globals()['single_nan_imp_'+str(i)].loc[j] = process(globals()['single_nan_'+str(i)].loc[[j]],mean_std,class_center)\n",
    "    \n",
    "    #---------------sending the single and multiple nan imputed values data to the final imputed data---------------#\n",
    "    final_imputed_data = final_imputed_data.append(globals()['single_nan_imp_'+str(i)])\n",
    "    final_imputed_data = final_imputed_data.append(globals()['multiple_nan_imp_'+str(i)])\n",
    "    final_imputed_data = final_imputed_data.sort_index()\n",
    "    final_imputed_data = final_imputed_data.round()\n",
    "    final_imputed_data = final_imputed_data.astype('int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9c071bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AE calculation\n",
    "numberOfCell = len(original_data.index) * len(original_data.columns)\n",
    "a = original_data - final_imputed_data\n",
    "numerator = (a == 0).sum()\n",
    "numerator = numerator.sum()\n",
    "AE = numerator/numberOfCell\n",
    "AE_values.append(AE)\n",
    "AE_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1582f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NRMS calculation\n",
    "nrms_numerator = np.linalg.norm(final_imputed_data - original_data)\n",
    "nrms_denomerator = np.linalg.norm(original_data)\n",
    "nrms = nrms_numerator/nrms_denomerator\n",
    "nrms_values.append(nrms)\n",
    "nrms_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358cb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97cad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221e421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
